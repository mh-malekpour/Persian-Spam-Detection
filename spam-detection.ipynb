{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <div style=\"color: black\">تمرین دوم: تشخیص هرزنامه ها</div>\n",
    "## <div style=\"color: blue\">Mohammad Hossein Malekpour | 9613425</div>\n",
    "_______________________________\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import hazm\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats, sparse\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import classification_report, confusion_matrix, balanced_accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    files = [file for file in os.listdir(path)]\n",
    "    emails = []\n",
    "    for file in files:\n",
    "        with open(path + file, 'r', encoding='utf-8') as txt_file:\n",
    "            emails.append(txt_file.read())\n",
    "    return emails\n",
    "    \n",
    "ham_test = read_data('./emails/hamtesting/')\n",
    "ham_train = read_data('./emails/hamtraining/')\n",
    "spam_test = read_data('./emails/spamtesting/')\n",
    "spam_train = read_data('./emails/spamtraining/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lable</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>﻿\\n-------------------------------------------...</td>\n",
       "      <td>spam</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>﻿\\nCompletely free tracking for websites &lt;http...</td>\n",
       "      <td>spam</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>﻿\\n   \\n  \\n\\nاتاقک : بهتـرین جامعـه مجـازی فا...</td>\n",
       "      <td>spam</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>﻿\\n-------------------------------------------...</td>\n",
       "      <td>spam</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>﻿\\n\\nazk2ylc0sxx2mk6qvp5y.jpg\\n&lt;http://www.8pi...</td>\n",
       "      <td>spam</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>﻿\\nبسمه تعالی\\n\\nسازمان زیباسازی شهرداری استان...</td>\n",
       "      <td>spam</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>﻿\\n\\nبه مناسبت فرا رسیدن میلاد دخت پیامبر گرام...</td>\n",
       "      <td>spam</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>﻿\\nدرود هموطن من\\n\\n \\n\\nتست رایگان   \\n\\n    ...</td>\n",
       "      <td>spam</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>﻿\\n\\n    *درج **لینک  در 8700 وبلاگ\\n    *\\n\\n...</td>\n",
       "      <td>spam</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>﻿\\nسلام به دوستان عزیز\\nشما هم میتوانید از این...</td>\n",
       "      <td>spam</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text lable  spam\n",
       "290  ﻿\\n-------------------------------------------...  spam     1\n",
       "291  ﻿\\nCompletely free tracking for websites <http...  spam     1\n",
       "292  ﻿\\n   \\n  \\n\\nاتاقک : بهتـرین جامعـه مجـازی فا...  spam     1\n",
       "293  ﻿\\n-------------------------------------------...  spam     1\n",
       "294  ﻿\\n\\nazk2ylc0sxx2mk6qvp5y.jpg\\n<http://www.8pi...  spam     1\n",
       "295  ﻿\\nبسمه تعالی\\n\\nسازمان زیباسازی شهرداری استان...  spam     1\n",
       "296  ﻿\\n\\nبه مناسبت فرا رسیدن میلاد دخت پیامبر گرام...  spam     1\n",
       "297  ﻿\\nدرود هموطن من\\n\\n \\n\\nتست رایگان   \\n\\n    ...  spam     1\n",
       "298  ﻿\\n\\n    *درج **لینک  در 8700 وبلاگ\\n    *\\n\\n...  spam     1\n",
       "299  ﻿\\nسلام به دوستان عزیز\\nشما هم میتوانید از این...  spam     1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ham_test_lable = {'text': ham_test, 'lable': ['ham' for i in range(len(ham_test))]}\n",
    "ham_train_lable = {'text': ham_train, 'lable': ['ham' for i in range(len(ham_train))]}\n",
    "spam_test_lable = {'text': spam_test, 'lable': ['spam' for i in range(len(spam_test))]}\n",
    "spam_train_lable = {'text': spam_train, 'lable': ['spam' for i in range(len(spam_train))]}\n",
    "\n",
    "train_data = pd.concat([pd.DataFrame(ham_train_lable), pd.DataFrame(spam_train_lable)])\n",
    "test_data = pd.concat([pd.DataFrame(ham_test_lable), pd.DataFrame(spam_test_lable)])\n",
    "\n",
    "train_data['spam'] = train_data['lable'].replace(['spam','ham'],[1, 0])\n",
    "test_data['spam'] = test_data['lable'].replace(['spam','ham'],[1, 0])\n",
    "\n",
    "train_data.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Proceess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "persian_punctuations = '''`÷×؛<>_()*&^%][ـ،/:\"؟.,'{}~¦+|!”…“–ـ'''\n",
    "english_punctuations = string.punctuation\n",
    "punctuations_list = persian_punctuations + english_punctuations\n",
    "\n",
    "arabic_diacritics = re.compile(\"\"\"\n",
    "                             ّ    | # Tashdid\n",
    "                             َ    | # Fatha\n",
    "                             ً    | # Tanwin Fath\n",
    "                             ُ    | # Damma\n",
    "                             ٌ    | # Tanwin Damm\n",
    "                             ِ    | # Kasra\n",
    "                             ٍ    | # Tanwin Kasr\n",
    "                             ْ    | # Sukun\n",
    "                             ـ     # Tatwil/Kashida\n",
    "                         \"\"\", re.VERBOSE)\n",
    "\n",
    "def remove_diacritics(text):\n",
    "    text = re.sub(arabic_diacritics, '', text)\n",
    "    return text\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    translator = str.maketrans('', '', punctuations_list)\n",
    "    return text.translate(translator)\n",
    "\n",
    "def remove_repeating_char(text):\n",
    "    return re.sub(r'(.)\\1+', r'\\1', text)\n",
    "\n",
    "def normalize_persian(text):\n",
    "    text = re.sub(\"[إأآا]\", \"ا\", text)\n",
    "    text = re.sub(\"ي\", \"ی\", text)\n",
    "    text = re.sub(\"ؤ\", \"و\", text)\n",
    "    text = re.sub(\"ئ\", \"ی\", text)\n",
    "    text = re.sub(\"ة\", \"ه\", text)\n",
    "    text = re.sub(\"ك\" ,\"ک\" , text)\n",
    "    text = re.sub(\"[^ابپتثجچحخدذرزژسشصضطظعغفقکگلمنوهی]\", \" \", text)\n",
    "    text = re.sub(\"[^\\S\\n\\t]+\", ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lable</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>اموزش زبان بهترین اموزش زبان با استفاده از صو...</td>\n",
       "      <td>spam</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>در مشاهده تصاویر مشکل دارید لطفا اینجا کلیک ک...</td>\n",
       "      <td>spam</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>اتاقک بهترین جامعه مجازی فارسی برای ایرانیان ...</td>\n",
       "      <td>spam</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>فروشگاه اینترنتی با بهترین محصولات برای تمامی...</td>\n",
       "      <td>spam</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>ایا می خواهید از وقت خود به بهترین نحو استفاد...</td>\n",
       "      <td>spam</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>بسمه تعالی سازمان زیباسازی شهرداری استان تهرا...</td>\n",
       "      <td>spam</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>به مناسبت فرا رسیدن میلاد دخت پیامبر گرامی اس...</td>\n",
       "      <td>spam</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>درود هموطن من تست رایگان تحویل اکانت پرداخت و...</td>\n",
       "      <td>spam</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>درج لینک در وبلاگ درج لینک و تبلیغات متنی شما...</td>\n",
       "      <td>spam</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>سلام به دوستان عزیز شما هم میتوانید از اینترن...</td>\n",
       "      <td>spam</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text lable  spam\n",
       "290   اموزش زبان بهترین اموزش زبان با استفاده از صو...  spam     1\n",
       "291   در مشاهده تصاویر مشکل دارید لطفا اینجا کلیک ک...  spam     1\n",
       "292   اتاقک بهترین جامعه مجازی فارسی برای ایرانیان ...  spam     1\n",
       "293   فروشگاه اینترنتی با بهترین محصولات برای تمامی...  spam     1\n",
       "294   ایا می خواهید از وقت خود به بهترین نحو استفاد...  spam     1\n",
       "295   بسمه تعالی سازمان زیباسازی شهرداری استان تهرا...  spam     1\n",
       "296   به مناسبت فرا رسیدن میلاد دخت پیامبر گرامی اس...  spam     1\n",
       "297   درود هموطن من تست رایگان تحویل اکانت پرداخت و...  spam     1\n",
       "298   درج لینک در وبلاگ درج لینک و تبلیغات متنی شما...  spam     1\n",
       "299   سلام به دوستان عزیز شما هم میتوانید از اینترن...  spam     1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['text'] = train_data['text'].apply(remove_diacritics)\n",
    "train_data['text'] = train_data['text'].apply(remove_punctuations)\n",
    "train_data['text'] = train_data['text'].apply(remove_repeating_char)\n",
    "train_data['text'] = train_data['text'].apply(normalize_persian)\n",
    "test_data['text'] = test_data['text'].apply(remove_diacritics)\n",
    "test_data['text'] = test_data['text'].apply(remove_punctuations)\n",
    "test_data['text'] = test_data['text'].apply(remove_repeating_char)\n",
    "test_data['text'] = test_data['text'].apply(normalize_persian)\n",
    "\n",
    "train_data.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Word Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lable</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>[بسمه, تعالی, سازمان, زیباسازی, شهرداری, استان...</td>\n",
       "      <td>spam</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>[به, مناسبت, فرا, رسیدن, میلاد, دخت, پیامبر, گ...</td>\n",
       "      <td>spam</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>[درود, هموطن, من, تست, رایگان, تحویل, اکانت, پ...</td>\n",
       "      <td>spam</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>[درج, لینک, در, وبلاگ, درج, لینک, و, تبلیغات, ...</td>\n",
       "      <td>spam</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>[سلام, به, دوستان, عزیز, شما, هم, میتوانید, از...</td>\n",
       "      <td>spam</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text lable  spam\n",
       "295  [بسمه, تعالی, سازمان, زیباسازی, شهرداری, استان...  spam     1\n",
       "296  [به, مناسبت, فرا, رسیدن, میلاد, دخت, پیامبر, گ...  spam     1\n",
       "297  [درود, هموطن, من, تست, رایگان, تحویل, اکانت, پ...  spam     1\n",
       "298  [درج, لینک, در, وبلاگ, درج, لینک, و, تبلیغات, ...  spam     1\n",
       "299  [سلام, به, دوستان, عزیز, شما, هم, میتوانید, از...  spam     1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['text'] = train_data['text'].apply(lambda txt: txt.split())\n",
    "test_data['text'] = test_data['text'].apply(lambda txt: txt.split())\n",
    "\n",
    "train_data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Remove Stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lable</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>[بسمه, تعالی, سازمان, زیباسازی, شهرداری, استان...</td>\n",
       "      <td>spam</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>[مناسبت, فرا, میلاد, دخت, پیامبر, گرامی, اسلام...</td>\n",
       "      <td>spam</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>[درود, هموطن, تست, رایگان, تحویل, اکانت, پرداخ...</td>\n",
       "      <td>spam</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>[درج, لینک, وبلاگ, درج, لینک, تبلیغات, متنی, ب...</td>\n",
       "      <td>spam</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>[سلام, دوستان, عزیز, هم, میتوانید, اینترنت, ری...</td>\n",
       "      <td>spam</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text lable  spam\n",
       "295  [بسمه, تعالی, سازمان, زیباسازی, شهرداری, استان...  spam     1\n",
       "296  [مناسبت, فرا, میلاد, دخت, پیامبر, گرامی, اسلام...  spam     1\n",
       "297  [درود, هموطن, تست, رایگان, تحویل, اکانت, پرداخ...  spam     1\n",
       "298  [درج, لینک, وبلاگ, درج, لینک, تبلیغات, متنی, ب...  spam     1\n",
       "299  [سلام, دوستان, عزیز, هم, میتوانید, اینترنت, ری...  spam     1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = hazm.stopwords_list()\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    for token in tokens:\n",
    "        if token in stopwords:\n",
    "            tokens.remove(token)\n",
    "    return tokens\n",
    "            \n",
    "train_data['text'] = train_data['text'].apply(remove_stopwords)\n",
    "test_data['text'] = test_data['text'].apply(remove_stopwords)\n",
    "\n",
    "train_data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Stemming & lemmatization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lable</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>[, تماس, داخل, حقیق, قرینه, مید, جه, ارتباط, ت...</td>\n",
       "      <td>spam</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>[, هست, هدیه, گرد, کلیک, امید, شما, اسلا, انشا...</td>\n",
       "      <td>spam</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>[, فقط, سرع, تس, افتاح, شما, رایگ, اکان, ایجاد...</td>\n",
       "      <td>spam</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>[, درصد, به, اصل, یکساله, تماس, بلاگساز, وبلاگ...</td>\n",
       "      <td>spam</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>[, طرح, به, گوگل, سرمایه, نمیخواهید, کرو, گف, ...</td>\n",
       "      <td>spam</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text lable  spam\n",
       "295  [, تماس, داخل, حقیق, قرینه, مید, جه, ارتباط, ت...  spam     1\n",
       "296  [, هست, هدیه, گرد, کلیک, امید, شما, اسلا, انشا...  spam     1\n",
       "297  [, فقط, سرع, تس, افتاح, شما, رایگ, اکان, ایجاد...  spam     1\n",
       "298  [, درصد, به, اصل, یکساله, تماس, بلاگساز, وبلاگ...  spam     1\n",
       "299  [, طرح, به, گوگل, سرمایه, نمیخواهید, کرو, گف, ...  spam     1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stemming(tokens):\n",
    "    stemmer = hazm.Stemmer()\n",
    "    s_tokens = set()\n",
    "    for token in tokens:\n",
    "        s_tokens.add(stemmer.stem(token))\n",
    "    return list(s_tokens)\n",
    "    \n",
    "def lemmatization(tokens):\n",
    "    lemmatizer = hazm.Lemmatizer()\n",
    "    l_tokens = set()\n",
    "    for token in tokens:\n",
    "        l_tokens.add(lemmatizer.lemmatize(token))\n",
    "    return list(l_tokens)\n",
    "\n",
    "train_data['text'] = train_data['text'].apply(stemming)\n",
    "test_data['text'] = test_data['text'].apply(lemmatization)\n",
    "\n",
    "train_data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_words = set()\n",
    "for text in train_data['text']:\n",
    "    for word in text:\n",
    "        train_words.add(word)\n",
    "        \n",
    "test_words = set()\n",
    "for text in test_data['text']:\n",
    "    for word in text:\n",
    "        test_words.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_data.text.to_list()\n",
    "y_train = train_data.lable.to_list()\n",
    "x_test = test_data.text.to_list()\n",
    "y_test = test_data.lable.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_vectorize(words, data):\n",
    "    word_index = dict()\n",
    "    for index, word in enumerate(words):\n",
    "        word_index[word] = index\n",
    "    DF = {}\n",
    "    for tokens in data:\n",
    "        distinct_tokens = set(tokens)\n",
    "        for token in distinct_tokens:\n",
    "            if token not in DF:\n",
    "                DF[token] = 0\n",
    "            DF[token] += 1\n",
    "    row = list()\n",
    "    col = list()\n",
    "    vector = list()\n",
    "    for index, text in enumerate(data):\n",
    "        WF = {}\n",
    "        for word in text:\n",
    "            if word in words:\n",
    "                if word not in WF:\n",
    "                    WF[word] = 0\n",
    "                WF[word] += 1\n",
    "        for word in WF.keys():\n",
    "            row.append(index)\n",
    "            col.append(word_index[word])\n",
    "            tfidf = (int(((np.log10(WF[word]) + 1) * (np.log10(len(data) / DF[word]))) * 100)) / 100\n",
    "            vector.append(tfidf)    \n",
    "    result = sparse.csr_matrix((vector, (row, col)), shape=(len(data), len(words))).toarray()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_vector = tfidf_vectorize(train_words, x_train)\n",
    "x_test_vector = tfidf_vectorize(train_words, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.09, 0.  , 0.  , ..., 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , ..., 0.  , 0.  , 0.  ],\n",
       "       [0.09, 0.  , 0.  , ..., 0.  , 0.  , 0.  ],\n",
       "       ...,\n",
       "       [0.09, 0.  , 0.  , ..., 0.  , 0.  , 0.  ],\n",
       "       [0.09, 0.  , 0.  , ..., 0.  , 0.  , 0.  ],\n",
       "       [0.09, 0.  , 0.  , ..., 0.  , 0.  , 0.  ]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction with Chi-Squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = SelectKBest(chi2, k = 500).fit(x_train_vector, y_train)\n",
    "x_train_vector_best = transformer.transform(x_train_vector)\n",
    "x_test_vector_best = transformer.transform(x_test_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Implementation with Cosine Similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim_knn(k, x_train, y_train, x_test):\n",
    "    sim = cosine_similarity(x_test, x_train)\n",
    "    result = list()\n",
    "    for i in range(len(sim)):\n",
    "        nearests = np.argsort(sim[i])[::-1][:k]\n",
    "        nearest_lables = list()\n",
    "        for n in nearests:\n",
    "            nearest_lables.append(y_train[n])\n",
    "        tmp = stats.mode(nearest_lables)[0]\n",
    "        result.append(tmp)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Implementation with TFIDF Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_knn(k, x_train, y_train, x_test):\n",
    "    train = list()\n",
    "    for text in x_train:\n",
    "        tmp = list()\n",
    "        for tfidf in text:\n",
    "            tmp.append(tfidf)\n",
    "        train.append(sum(tmp))\n",
    "        \n",
    "    test = list()\n",
    "    for text in x_test:\n",
    "        tmp = list()\n",
    "        for tfidf in text:\n",
    "            tmp.append(tfidf)\n",
    "        test.append(sum(tmp))\n",
    "        \n",
    "    scores = list()\n",
    "    for x in test:\n",
    "        tmp = list()\n",
    "        for y in train:\n",
    "            tmp.append(abs(x - y))\n",
    "        scores.append(tmp)\n",
    "\n",
    "    result = list()\n",
    "    for idx in range(len(scores)):\n",
    "        nearests = np.argsort(scores[idx])[:k]\n",
    "        nearest_lables = list()\n",
    "        for n in nearests:\n",
    "            nearest_lables.append(y_train[n])\n",
    "        tmp = stats.mode(nearest_lables)[0]\n",
    "        result.append(tmp)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier Train & Comparison & Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN with CosineSimilarities (main words):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN with Cosine Similarities\n",
      "best k:3\n",
      "accuracy:0.955\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.95      0.95      0.95       200\n",
      "        spam       0.95      0.95      0.95       200\n",
      "\n",
      "    accuracy                           0.95       400\n",
      "   macro avg       0.95      0.95      0.95       400\n",
      "weighted avg       0.95      0.95      0.95       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_acc = 0.0\n",
    "for k in range(2, 50):\n",
    "    y_pred = cos_sim_knn(k, x_train_vector, y_train, x_test_vector)\n",
    "    acc = balanced_accuracy_score(y_test, y_pred)\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_k = k\n",
    "print(f'KNN with Cosine Similarities\\nbest k:{best_k}\\naccuracy:{best_acc}\\n')\n",
    "y_pred = cos_sim_knn(best_k, x_train_vector, y_train, x_test_vector)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN with TFIDF Score (main words):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN with TFIDF Score\n",
      "best k:5\n",
      "accuracy:0.6325000000000001\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.60      0.78      0.68       200\n",
      "        spam       0.69      0.49      0.57       200\n",
      "\n",
      "    accuracy                           0.63       400\n",
      "   macro avg       0.64      0.63      0.62       400\n",
      "weighted avg       0.64      0.63      0.62       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_acc = 0.0\n",
    "for k in range(2, 50):\n",
    "    y_pred = tfidf_knn(k, x_train_vector, y_train, x_test_vector)\n",
    "    acc = balanced_accuracy_score(y_test, y_pred)\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_k = k\n",
    "print(f'KNN with TFIDF Score\\nbest k:{best_k}\\naccuracy:{best_acc}\\n')\n",
    "y_pred = tfidf_knn(best_k, x_train_vector, y_train, x_test_vector)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN with CosineSimilarities (chi2 - best words):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN with Cosine Similarities\n",
      "best k:4\n",
      "accuracy:0.94\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.92      0.96      0.94       200\n",
      "        spam       0.96      0.92      0.94       200\n",
      "\n",
      "    accuracy                           0.94       400\n",
      "   macro avg       0.94      0.94      0.94       400\n",
      "weighted avg       0.94      0.94      0.94       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_acc = 0.0\n",
    "for k in range(2, 50):\n",
    "    y_pred = cos_sim_knn(k, x_train_vector_best, y_train, x_test_vector_best)\n",
    "    acc = balanced_accuracy_score(y_test, y_pred)\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_k = k\n",
    "print(f'KNN with Cosine Similarities\\nbest k:{best_k}\\naccuracy:{best_acc}\\n')\n",
    "y_pred = cos_sim_knn(best_k, x_train_vector_best, y_train, x_test_vector_best)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN with TFIDF Score (chi2 - best words):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN with TFIDF Score\n",
      "best k:43\n",
      "accuracy:0.7425\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.68      0.93      0.78       200\n",
      "        spam       0.88      0.56      0.69       200\n",
      "\n",
      "    accuracy                           0.74       400\n",
      "   macro avg       0.78      0.74      0.73       400\n",
      "weighted avg       0.78      0.74      0.73       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_acc = 0.0\n",
    "for k in range(2, 50):\n",
    "    y_pred = tfidf_knn(k, x_train_vector_best, y_train, x_test_vector_best)\n",
    "    acc = balanced_accuracy_score(y_test, y_pred)\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_k = k\n",
    "print(f'KNN with TFIDF Score\\nbest k:{best_k}\\naccuracy:{best_acc}\\n')\n",
    "y_pred = tfidf_knn(best_k, x_train_vector_best, y_train, x_test_vector_best)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
